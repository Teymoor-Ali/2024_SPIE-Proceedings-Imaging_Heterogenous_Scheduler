\documentclass[]{spie}  %>>> use for US letter paper
%\documentclass[a4paper]{spie}  %>>> use this instead for A4 paper
%%\documentclass[nocompress]{spie}  %>>> to avoid compression of citations

\renewcommand{\baselinestretch}{1.0} % Change to 1.65 for double spacing

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{comment}
\usepackage{color}
\usepackage{tabularray}
\definecolor{Silver}{rgb}{0.752,0.752,0.752}
\definecolor{Mercury}{rgb}{0.901,0.901,0.901}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{pgfplots}
\usepackage{tikz}
\pgfplotsset{compat=1.17}
\pgfplotsset{compat=1.3}
\usepackage{booktabs}
\usepackage{hhline}
\usepackage{float}
\usepackage[table,xcdraw]{xcolor} % Needed for coloring tables
\usepackage{caption}
\usepackage{tabularray}





\definecolor{Gray1}{gray}{0.9}
\definecolor{Gray2}{gray}{0.8}
\definecolor{ppink}{HTML}{ec81fc}
\definecolor{oorange}{HTML}{eda477}
\definecolor{bbrown}{HTML}{826441}
\definecolor{peridot}{rgb}{0.9, 0.89, 0.0}
\definecolor{richlavender}{rgb}{0.67, 0.38, 0.8}
\definecolor{Silver}{rgb}{0.752,0.752,0.752}

\definecolor{forestgreen}{rgb}{0.0, 0.27, 0.13}
\definecolor{darkcandyapplered}{rgb}{0.64, 0.0, 0.0}
\definecolor{paleplum}{rgb}{0.8, 0.6, 0.8}
\definecolor{pastelorange}{rgb}{1.0, 0.7, 0.28}

\definecolor{yellow-green}{rgb}{0.6, 0.8, 0.2}
\definecolor{wenge}{rgb}{0.39, 0.33, 0.32}
\definecolor{corn}{rgb}{0.98, 0.93, 0.36}
\definecolor{darkpowderblue}{rgb}{0.0, 0.2, 0.6}
\definecolor{dimgray}{rgb}{0.41, 0.41, 0.41}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Scheduling algorithms on heterogeneous architecture for efficient vision systems }

\author[a]{Teymoor Ali. }
\author[a,b]{George Paul. }
\author[a]{Deepayan Bhowmik.}
\author[b]{Robert Nicol.}
\affil[a]{Newcastle University, 1 Science Square, Newcastle upon Tyne, U.K}
\affil[b]{STMicroelectronics, 1 Tanfield, Inverleith Row, Edinburgh, U.K}

\authorinfo{Further author information: (Send correspondence to T. Ali)\\T. Ali: E-mail: Teymoor.A@outlook.com, \\  G.P E-mail: George.Paul@newcastle.ac.uk.}

% Option to view page numbers
\pagestyle{empty} % change to \pagestyle{plain} for page numbers   
\setcounter{page}{301} % Set start page numbering at e.g. 301
 
\begin{document} 
\maketitle

\begin{abstract}
Deep Neural Network (DNN) algorithms have become ubiquitous within the vision domain, encompassing various tasks, including object detection, segmentation, and classification. However, executing complex DNN algorithms on real-time vision systems demands better energy efficiency, runtime, and accuracy requirements. The growing demand for image processing algorithms on systems with resource and energy constraints requires architectures that perform tasks efficiently. Traditionally, embedded imaging designs often involve implementing image processing algorithms on homogeneous architectures, which have hardware limitations. This has led to innovative computing architectures, leveraging heterogeneity that combines CPUs, GPUs, FPGAs, and other accelerators into a single processing fabric. This design choice allows applications to utilise the most efficient architecture for a given algorithm.

%In image processing, the advantages of GPUs facilitate the acceleration of highly parallel and memory-intensive tasks such as training and inference of convolutional neural networks (CNNs). On the other hand, FPGAs offer a reconfigurable architecture composed of configurable logic blocks (CLBs) interconnected by programmable routing resources, enabling custom designs tailored to specific imaging algorithms. These CLBs, consisting of look-up tables (LUTs), flip-flops, and multipliers, support a variety of logical and arithmetic operations, while dedicated memory blocks (BRAM) and digital signal processing (DSP) slices optimise multiply-and-accumulate operations.

However, scheduling and partitioning algorithms remain an arduous task, particularly when distributing operations among accelerators with different computing paradigms. This challenge requires a balance, considering important factors such as computational power, memory bandwidth, and communication overhead. In addition, scheduling tasks involves determining the order in which tasks are executed while considering factors such as task dependencies, resource availability, and synchronisation. Current state-of-the-art deep learning libraries are primarily designed to target single architectures for model deployment and often lack mechanisms to intelligently partition an algorithm's sub-operations across the most suitable processor. This paper presents a scheduler targeting heterogeneous vision systems, which carefully fine-grain partitions and maps sub-operations of widely used convolutional neural networks and image processing algorithms. Our experiments reveal that the scheduled partitioned algorithms perform better in energy and runtime efficiency than their best-performing homogeneous components executing the complete algorithm.
\end{abstract}

% Include a list of keywords after the abstract 
\keywords{Heterogeneous Computing, Computer Vision, FPGA-GPU, Heterogenous Scheduler}


\section{INTRODUCTION}
\label{sec:intro}
Vision systems have increasingly become pervasive in many application domains, particularly those utilising deep learning (DL) algorithms such as Satellites \cite{10.1145/2491956.2462176}, Autonomous Vehicles \cite{kuutti2020survey}, and Remote Sensing \cite{YUAN2020111716}. However, many of these systems face low-SWaP (Size, Weight, and Power) constraints, adding further complexity and limiting available resources. Traditionally, most hardware architectures are composed of homogeneous compute units coupled with fixed memory hierarchies, such as Central Processing Units (CPUs) or Graphics Processing Units (GPUs). While these architectures compute specific algorithms efficiently, in some cases, performance is traded off for flexibility, leading to poorer execution for operations that do not fit the design. This rigidity becomes particularly problematic when deploying deep learning models with diverse computational needs. The diverse operators within deep learning and image processing workloads, ranging from convolutional layers to recurrent networks, often exhibit varying levels of parallelism and memory access patterns.

Heterogeneous architectures have been shown as a potential path forward to keep up with the rising demand for better performance. This hardware paradigm integrates various compute accelerators to form a single compute fabric, sharing resources whilst leveraging their unique processing characteristics. Computer vision algorithms comprise various image processing and deep learning algorithms, forming a pipeline. GPUs, with their massively parallel architecture consisting of thousands of cores, streaming multiprocessors (SMs), and single instruction multiple data (SIMD) units, excel at accelerating pixel-level computations. Their specialised hardware, such as texture units for efficient image filtering and high-bandwidth memory interfaces for fast data access, enables fast execution of pixel processing streams. 

In addition, FPGAs bring their own unique advantages for imaging tasks due to their inherent flexibility and configurable architecture. Unlike fixed-function processors, FPGAs offer the capability to tailor specific image processing algorithms through hardware-level customisation, leveraging programmable logic blocks, custom memory hierarchy, and digital signal processing (DSP) slices. This level of customisation enables the development of highly optimised image processing pipelines finely tuned to specific requirements. Furthermore, FPGAs employ a versatile routing architecture that can be configured to efficiently manage dataflow between operations. This fine-grained control over routing allows data to be directed through the configurable logic fabric with minimal latency. This customisation capability allows for low-latency, real-time processing of image streams. Neural Processing Units often incorporate specialised hardware units, such as systolic arrays or matrix multiplication engines, optimised for the core mathematical operations found in deep neural networks.

Furthermore, as deep neural networks increase in architectural complexity and model size, they demand significant memory resources to store their weights, posing challenges for implementation on resource and energy constrained platforms. Despite these challenges, true heterogeneous computing offers a pathway to developing runtime and power-efficient designs. By exploiting architectures with sufficient resources and processing capabilities, it is possible to achieve high-performance computation while adhering to the constraints of resource-limited environments.

Deep learning frameworks such as TensorFlow\cite{tensorflow2015} and PyTorch\cite{Pytorch} provide high-level APIs and abstractions to simplify model development and deployment. These frameworks utilise graph-based intermediate representations (IR) like XLA or TorchScript to enable optimisations and portability across different hardware platforms. Machine learning compilers like TVM\cite{chen2018} or MLIR\cite{mlir} further optimise these IRs for specific targets, leveraging techniques such as operator fusion, memory layout optimisation, and kernel specialisation. However, the current tools often revolve around a single backend, such as CUDA for GPUs. While this simplifies development for specific hardware, it limits the ability to exploit the heterogeneity.


In this paper, we develop a scheduler that partitions computer vision pipelines and their sub-algorithms into suitable architectures, optimising performance and energy efficiency. Subgraphs are generated from PyTorch for each selected pipeline, allowing distinct segments of the algorithms to be independently executed on various accelerators, thus leveraging the strengths of heterogeneous systems. We evaluate the performance of these partitioned pipelines and compare them to their discrete accelerator counterparts, analysing key metrics such as inference time, energy consumption, and accuracy. Additionally, the scheduler's performance is benchmarked against other ML compilers, such as TVM. The feasibility and benefits of this approach offer key insights into optimal deployment strategies for computer vision pipelines on heterogeneous hardware platforms.

% Convolutional Neural Networks (CNNs)  are prevalent in various domains, including object detection\cite{zhao2012flip}, image classification\cite{rawat2017deep}, and image segmentation\cite{minaee2021image}. These image processing algorithms are generally designed and implemented on GPUs, which feature thousands of compute cores coupled with high-bandwidth memory. This configuration enables efficient execution of Single Instruction, Multiple Data (SIMD) operations, making GPUs ideal for parallel processing of extensive data sets. However, the execution of algorithms on GPUs involves trade-offs in power consumption, physical size, and latency\cite{PouSamSad}.



The main contributions of this paper are as follows:
\begin{itemize}
\item Development of a heterogeneous scheduler for computer vision pipelines, enabling task allocation to a selected compute unit and facilitating direct memory access operations between GPU and FPGA.
\item Optimised deployment of computer vision pipelines on heterogeneous architectures, demonstrating improved throughput and reduced energy consumption compared to homogeneous accelerator solutions.
\item Comprehensive benchmarking and evaluation of algorithms generated by machine learning frameworks (ONNX, PyTorch) across a heterogeneous system, including CPUs, GPUs, and FPGAs.
\end{itemize}

\newpage


\section{RELATED WORK}
\label{sec:background}



\textbf{Heterogeneous Architectures}. have seen increasing attention towards as an alternative to the limitations of homogeneous silicon chips \cite{RooLav17, KobRyoFuj20, XieLinKai17, ChoLeeLee22, Hosseinabady2019HeterogeneousFE}. Previous studies \cite{QasDenKri29} indicate that not all algorithms are suited to a single type of accelerator. These studies compare the energy efficiency of different architectures for image processing tasks and reveal that GPUs generally consume less energy per frame than CPUs and FPGAs. However, for more complex kernels and complete vision pipelines, FPGAs outperform other accelerators. In addition, CNNs executed on FPGAs are more efficient than those on GPUs for inference tasks, both in terms of energy consumption and, in some cases, processing time, as evidenced by Blott et al\cite{BloHalLis}.



\noindent\textbf{Partitioning.} algorithms for heterogeneous architectures require careful balancing diverse of computing elements and managing resource dependencies\cite{shekhar2015}. one work introduced StreamBlocks, a software based FPGA compiler that simplifies partitioning tasks across heterogeneous CPU/FPGA platforms but noted the difficulty of achieving an optimal division without extensive profiling and code modifications \cite{emami2022}. Another study addressed the problem of optimising data-parallel applications on heterogeneous HPC platforms by proposing a model-based partitioning algorithm that tackles the intricacies of resource contention and NUMA effects, although this algorithm requires detailed profiling \cite{khaleghzadeh2018}. Further research emphasised the energy and performance trade-offs in workload partitioning, demonstrating that effective utilisation of heterogeneous processors requires sophisticated balancing strategies \cite{tang2017}. Other research has explored prioritising the energy efficiency of accelerators when partitioning tasks in heterogeneous computing environments. Hamano et al.\cite{Hamano} proposed a power-aware dynamic task allocation strategy for heterogeneous clusters of CPUs and GPUs, aiming to minimise the Energy-Delay Product (EDP). The approach prioritises energy efficiency by initially assigning tasks to the lowest-power resources and then selectively offloading tasks with a high acceleration potential to other resources to reduce processing time. However, this method may lead to excessive energy consumption when minimising processing time, even if the application does not require instant processing. Task partitioning offers a more fine-grained scheduling approach by dividing tasks into sub-tasks and assigning them to suitable resources. Inta et al.\cite{IntBowDav12} presented a static partitioning method for FPGA-GPU hybrid systems. However, such methods can introduce additional delays due to the processing time differences between FPGAs and GPUs, potentially causing the system to miss service deadlines. Oh et al.\cite{OhHanYan18} improved the scheduling method to consider both energy consumption and compliance with the latency limitation of
the application at the same time.

\noindent\textbf{Scheduling.} tasks across heterogeneous systems come with additional challenges related to communication and memory latency. Scheduling involves efficiently allocating computational tasks across various processing units such as CPUs, GPUs, FPGAs, and other specialised hardware. This scheduling aims to optimise performance, resource utilisation, and energy efficiency by leveraging the distinct strengths of each hardware type. Several works have explored the deployment of algorithms for heterogeneous architectures on edge and HPC domain\cite{KheHardMue24,RatGouHog23}.  Nupur et al\cite{SumRaw22}, presented a optimal partitioning approach to map a CNN model across heterogeneous architectures by leveraging performance measurement benchmarks. The proposed framework, Hetero-vis, decides optimal partitions and mapping of a CNN network across different accelerators to minimise the inference latency. Despite progress, current deep learning libraries often target one backend, lacking mechanisms to partition and distribute tasks across various processors intelligently. This gap requires the development of schedulers that can dynamically allocate resources based on the computational requirements and available hardware\cite{KanLeeKil21, LanBhaSou2016}.

\noindent\textbf{Compiler Frameworks}. such as TVM and PyTorch, employ sophisticated techniques for optimising and transforming deep learning models. TVM utilises graph optimisation, kernel fusion, and hardware-specific code generation to execute efficiently across various hardware platforms \cite{chen2018}. Similarly, PyTorch integrates dynamic computational graphs with JIT compilation and custom kernel optimisation to enhance model performance on diverse architectures \cite{Pytorch}. These compilers aim to bridge the gap between high-level model specification and efficient hardware execution, improving model inference speed and resource utilisation. A framework utilising a polyhedral model was proposed by Baghdadi et al.\cite{BagRiyRay19} to generate high-performance code for many backends. Its design separates computation into four intermediate representations: algorithms, loop transformations, data layouts, and communication. Other approaches employ a two-phase search algorithm, hierarchical sampling, and memorisation of repeated partial schedules to address scalability challenges. These optimisation results have shown compile-time speedups and performance improvement\cite{Halide2018}.


\section{PIPELINE AND SCHEDULER DESIGN}
\label{sec:methodology}

This section details the heterogeneous deployment of computer vision pipelines that combine OpenCV-based image processing with a Convolutional Neural Network (CNN) classifier, utilising PyTorch as the framework for describing the pipeline. 

The computational graph generated from the PyTorch algorithm includes both the image processing and classifier stages. The graph is manually partitioned by the programmer to allocate tasks across the GPU and FPGA, with both accelerators being utilised for any task based on availability. This approach involves executing image processing on either the GPU or FPGA and performing inference on whichever accelerator is free, using Direct Memory Access (DMA) to efficiently transfer processed image frames between the GPU and FPGA. The primary objective is to optimise runtime, frames per second (FPS), latency, and energy consumption, demonstrating that this heterogeneous approach is superior to the traditional homogeneous approach where both image processing and inference are executed entirely on GPU or FPGA.

\subsection{Pipeline Overview}

The computer vision pipeline begins with an image processing module using OpenCV functions, followed by a CNN classifier. Both stages are implemented as a single module in PyTorch. The deployment flow is illustrated in Figure \ref{fig:deployment_flow}.

The scheduler splits the graph into sub-operations, assigning them to the most suitable hardware units based on their computational characteristics. In addition, the scheduler is integrated into a deployment framework that works with PyTorch, ONNX Runtime, and the vendor compilers for the target hardware including NVCC and Vitis compilers. Furthermore, peer-to-peer direct memory transfer (DMA) is injected into the generated code to transfer image data from FPGA to GPU buffers without involving the host (CPU). The FPGA scheduling flow takes longer when algorithms have to be synthesised in cases where pre-written IP is not used.

% Perhaps table for TVM vs PyTorch to justify our decision
Although TVM 0.6 can process ONNX computation graphs, when it was tested as part of the scheduler it performed slower in throughput than PyTorch. TVM incurs significant overhead due to its intermediate representation and compilation steps, which may not be optimised for dynamic control flows. Additionally, TVM may struggle with kernel fusion in complex pipelines. Moreover, while TVM generally optimises CNN channels using the GEMM implementation for 2-D convolution, there are frequent instances where it opts for direct convolution, which is typically slower regardless of the target hardware characteristics \cite{9042000}. Furthermore, TVM may not fully support newer GPU architectures, limiting its efficiency and requiring additional tuning to achieve optimal performance.


\begin{figure}[tb]
\centering
\includegraphics[width=\textwidth]{images/SPIE_Deployment_Flow.png} 
\caption{Scheduler Architecture to Partition and Schedule Algorithms onto CPU, GPU, and FPGA.}
\label{fig:deployment_flow}
\end{figure}

\subsection{Pipeline Conversion and Deployment}

Initially, the pipeline is implemented and validated in PyTorch, including image processing with OpenCV functions and a CNN for classification. To optimise execution on CPUs and GPUs, the PyTorch module is converted to TorchScript, which supports various graph-level optimisations while preserving the model structure for efficient execution.

The module is exported to the Open Neural Network Exchange (ONNX) format for deployment on FPGAs. ONNX standardises the model representation, facilitating interoperability across different hardware platforms and leveraging FPGAs' reconfigurable architecture. Additionally, the model undergoes quantisation to reduce precision, optimising power and memory usage, which is particularly beneficial for resource-constrained devices like FPGAs.

\subsection{Heterogeneous Scheduler}
\begin{algorithm}[]
\caption{Heterogeneous Scheduler}
\begin{algorithmic}[1]

\STATE \textbf{function} Scheduler(PythonCode)
\STATE \hspace{1em} Parse PythonCode for pragmas (\texttt{gpu}, \texttt{fpga})
\STATE \hspace{1em} PartitionedCode $\leftarrow$ []

\STATE \hspace{1em} \textbf{for each} function \textbf{in} PythonCode:
\STATE \hspace{2em} \textbf{if} pragma == \texttt{gpu} \textbf{or} pragma == \texttt{fpga}:
\STATE \hspace{3em} \textbf{if} function in PerformanceCost:
\STATE \hspace{4em} \textbf{if} PerformanceCost[function] == \texttt{gpu}:
\STATE \hspace{5em} GeneratedCode $\leftarrow$ GenerateGPUCode(function)
\STATE \hspace{4em} \textbf{else if} PerformanceCost[function] == \texttt{fpga}:
\STATE \hspace{5em} \textbf{if} PreOptimizedLibraryExists(function):
\STATE \hspace{6em} GeneratedCode $\leftarrow$ UsePreOptimizedLibrary(function)
\STATE \hspace{5em} \textbf{else}:
\STATE \hspace{6em} ONNXCode $\leftarrow$ ConvertToONNX(function)
\STATE \hspace{6em} GeneratedCode $\leftarrow$ CompileWithVitis(ONNXCode)
\STATE \hspace{5em} DMAConfig $\leftarrow$ AddDMAToFPGA(GeneratedCode)
\STATE \hspace{4em} Append PartitionedCode with GeneratedCode and DMAConfig
\STATE \hspace{1em} \textbf{return} PartitionedCode

\end{algorithmic}
\end{algorithm}


The scheduler is designed to partition Python code and execute operations on GPU and FPGA. To reduce the host communication overhead, the algorithm incorporates remote direct memory access\cite{xilinx_2019, nvidia_2024}. It starts by parsing the input Python code to detect pragmas, specifically looking for \texttt{\#GPU} or \texttt{\#FPGA} directives that indicate the preferred execution platform for each function. If no directives are present, the algorithm defaults to using an internal cost model file, which includes a list of OpenCV functions and specifies the most suitable accelerator for each function. The scheduler employs a FIFO (First In, First Out) approach to ensure that the algorithms are executed in the order they appear in the Python code. As the scheduler parses the input code, it sequentially processes each function according to its pragma directives or performance cost evaluations. The first algorithm encountered is executed first, and the resulting data is then passed on to the next algorithm in line. This method ensures a streamlined and orderly execution flow, where the output from one function serves as the input for the subsequent function, maintaining data continuity and reducing complex data dependency.

For functions marked with a \texttt{\#GPU} pragma, or according to the performance cost evaluation, the scheduler invokes Pytorch to process the computational graph to generate GPU executable code.
On the other hand, for functions marked with an \texttt{\#FPGA} pragma, the algorithm checks for the availability of pre-optimised libraries (E.g Vitis Vision Library). If a pre-optimised library is found, the \texttt{UsePreOptimizedLibrary} function is called to retrieve and use the library code, thereby reducing compilation and synthesis time. If no such function exists in the library, the code is converted to ONNX computational graph format to be used with TVM flow. The computational graph generated from TVM is synthesised using the Vitis compiler to produce register-transfer-level logic.
DMA logic and configurations are also manually added to both FPGA and GPU. This step is required as it ensures high-speed data transfer with minimal CPU involvement, leveraging DMA's ability to move data directly between memory and peripherals. 

\subsection{Algorithm Profiling \& Partitioning Strategy}
To find the most optimal accelerator, each sub-algorithm within a computer vision is profiled manually to develop a primitive cost model. Each algorithm is executed on a CPU, GPU and FPGA and their profiling runtime, FPS and energy consumption for each hardware accelerator are recorded in Figure \ref{fig:IndividualRuntime}, Figure \ref{fig:FPSEnergy} and Table \ref{tab:Individualsummary}. 

\begin{figure}
    \centering
\resizebox{0.7\linewidth}{!}{\input{graphs/PartitionStrat}}    %\caption{Execution times for individual and combined algorithms.}
    \caption[SIFT Power Consumption]{Kernel (Lighter Colour) and Total Runtime (Solid Colour) Comparison of Individual Algorithms and Pipelines on each Hardware Platform. CPU(5900X) GPU(A2000) FPGA(Alveo U50).}
    \label{fig:IndividualRuntime}
\end{figure}

In terms of individual algorithm kernel runtimes shown in Figure \ref{fig:IndividualRuntime}, the results reveal that CPUs suffer poorly on high complexity algorithms which require more parallelisation. However, algorithms such as \texttt{RGB2Gray} and \texttt{Resize} exhibit better performance on the CPU than GPU. The lower GPU runtime can be attributed to communication latency when allocating cores takes longer than the processing. The FPGA is shown to perform $1.81\times$ faster than the GPU and $1.79\times$ faster than the CPU in terms of kernel time for \texttt{RGB2GRAY}, \texttt{ Histogram Equalisation} and \texttt{Resize}) algorithms. However, the FPGA is less efficient for the \texttt{Gaussian} Filter algorithm, performing $1.2\times$ slower than the GPU. This inefficiency arises due to the limited number of logic blocks available on the FPGA, which are insufficient to fully parallelise the processing of all pixels. As a result, the FPGA cannot leverage its parallel processing capabilities to the same extent as the GPU, leading to longer kernel runtimes. This highlights a critical trade-off point where the FPGA's hardware constraints limit its performance on more demanding algorithms. In the case of deep learning algorithms, the GPU is $4.21\times$ \& $1.41\times$ faster than the FPGA and CPU, respectively. The convolutional layer, which contains the highest intensity of the operations in a CNN, benefits from the many-core GPU architecture. 

Considering total execution time, the GPU performs poorly, taking in device initialisation, communication, and memory latency overheads. 

This is especially shown for low-complexity algorithms where parallelisation cannot be effectively exploited. On the other hand, the bitstream pre-initialised U50 FPGA waits for incoming data, resulting in minimal latency, which can benefit image sensors streaming pixel data. In addition, the model initialisation takes, on average, $150ms~$ on both CPU and GPU. Moreover, the U50 employs high-bandwidth memory, which can further improve memory-bound algorithms instead of the GDDR6 of the A2000 GPU. 

According to Figure \ref{fig:FPSEnergy}, the Frames Per Second (FPS) results show the impact of CNN algorithms. Large CNN architectures (E.g. Resnet-50, InceptionV3) with deeper layers will involve a lot more operations, additional processing modules and residual connections will also affect overall performance. In relation to energy consumption, the FPGA consumed the least amount of energy per operation for all algorithms. On average, the FPGA is $7.63\times$ and $2.60\times$ more energy efficient than the CPU and GPU. Although the GPU has a higher thermal design power (TDP) than the CPU, the time it takes to execute the algorithm is much lower, resulting in lower overall energy use despite the higher power draw. Algorithms that contain many operations, such as ML and Gaussian, show that the GPU consumes less energy than the CPU.

Referring to the profiling results, the partitioning approach will place the pre-processing algorithms on the FPGA to maximise energy efficiency, while the deep learning algorithm will be deployed on the GPU for runtime. This strategy takes advantage of the FPGA's low initialisation latency and memory coupled with the GPUs parallel processing capability.




\begin{figure}[tb]
    \centering
    \begin{tabular}{cc}
    \resizebox{0.5\columnwidth}{!}{\input{graphs/IndividualEnergy}} &
    \resizebox{0.5\columnwidth}{!}{\input{graphs/IndividualFPS}} \\    a & b 
    \end{tabular}
    \caption{Algorithmic a) Energy Consumption Comparison of Individual Algorithms, Pipelines and b) Frames per Second on each Hardware Platform. CPU(5900x) GPU(A2000) FPGA(Alveo U50).}
    \label{fig:FPSEnergy}
\end{figure}



% \begin{figure}
%     \centering
% \resizebox{0.7\linewidth}{!}{\input{graphs/IndividualEnergy}}    %\caption{Execution times for individual and combined algorithms.}
%     \caption[SIFT Power Consumption]{Energy Consumption Comparison of Individual Algorithms and Pipelines on each Hardware Platform.}
%     \label{fig:IndividualPower}
% \end{figure}




\section{Experimental Setup}
\input{tables/PlatformSummary} 
The proposed partitioning is evaluated using a heterogeneous platform containing various hardware components and their software, as shown in Table \ref{tab:HWEnvironment} and Figure \ref{fig:HeterogenousHardware}.

\textbf{Dataset.} The test images used in the experiments are from LIU4K-v2 dataset \cite{LiuliuYan19}, which is a high resolution data-set that includes 2000 $3840\times2160$ images. The images contain a variety of backgrounds and objects. 


\subsection{Measurement Metrics}
\subsubsection{Execution time}
The evaluation of the overall system performance considers both latency and compute factors, reporting performance metrics for total time, inference, and other significant layers while using floating point 16bit precision. The run-time is measured using the host platform's built-in time libraries. The algorithm runtime is collected from executing $1000\times$ and averaged.  

The U50 FPGA, time measurements are captured using the Xilinx Vitis analyser built-in performance counters. On the GPU, CUDA events are utilised to measure time. CUDA events work by recording timestamps at specific points in the code. These timestamps are captured by calling \verb|cudaEventRecord()| at the desired start and stop points of the code. The \verb|cudaEventElapsedTime()| function is then used to calculate the elapsed time between these two events. CUDA events provide high-resolution timing by using the GPU's internal clock, allowing for precise measurement of kernel execution times and other GPU operations with minimal overhead. For the CPU, a C++ library is employed to measure execution time. This library offers high-resolution timing functions such as \verb|std::chrono::high_resolution_clock| to capture start and end times, providing precise timing information for the CPU operations. The frame per second (FPS) metric is computed using Eq. \ref{eq:FPS}: 

\begin{equation}\label{eq:FPS}
\text{FPS}= 1/\text{Execution Time}.
\end{equation}

\subsubsection{Power Consumption}

Taking the instantaneous power or TDP of a device is not accurate since power consumption varies on the specific workload. Therefore, measuring power over the time it takes for the algorithm to execute improves accuracy as opposed to using just fixed Wattage. A script is developed to start and stop the measurements during the algorithm's execution. The mean wattage is averaged and multiplied by the time to determine the energy consumed in Joules (J). The energy consumption is obtained using Eq. (\ref{eq:Energy}), where \textit{E} is energy, \textit{P} represents power, and \textit{t} time.

\begin{equation}\label{eq:Energy}
E = P \times t
\end{equation}

Power measurements are taken using software based platform-specific tools. For the CPU, \texttt{RAPL} is used to collect energy consumption by using a script which reads the initial and final energy values. For the GPU, \texttt{nvidia-smi} is employed to obtain power readings at the same interval. For the U50 FPGA, \texttt{xbutil} is used to monitor power consumption. The power readings are then averaged to provide an accurate measure of energy consumption during the algorithm's execution.

\subsubsection{Accuracy}
The accuracy of the proposed system for CNN classification is evaluated using a straightforward metric: the ratio of correctly classified images to the total number of images in the test dataset. This metric provides a clear measure of how well the CNN model performs in terms of correctly identifying the class labels of the input images. The accuracy is calculated using Eq. \ref{eq
}:

\begin{equation}\label{eq
}
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
\end{equation}

RMSE is used to compare the accuracy of the output images from each algorithm, across hardware platforms. RMSE quantifies the differences between predicted and actual pixel values in image processing tasks. It is calculated using Eq. \ref{RSME}:

\begin{equation}\label{RSME}
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (I_{pred}(i) - I_{actual}(i))^2}
\end{equation}

where \( N \) is the total number of pixels, \( I_{\text{pred}}(i) \) is the predicted pixel value, and \( I_{\text{actual}}(i) \) is the actual pixel value.



\section{Results}
\label{sec:results}

% \begin{figure}
%     \centering
% \resizebox{0.7\linewidth}{!}{\input{graphs/HeterogenousResults}}  
%     \caption{Kernel and Total Runtime  Comparison of Individual Algorithms and Pipelines on each Hardware Platform. CPU(5900X) GPU(A2000) FPGA(Alveo U50).}
%     \label{fig:HeterogeneousRuntime}
% \end{figure}

\begin{figure}[tb]
    \centering
    \begin{tabular}{cc}
    \resizebox{0.48\columnwidth}{!}{\input{graphs/HeterogenousResults}} &
    \resizebox{0.49\columnwidth}{!}{\input{graphs/HeterogenousEnergy}} \\    a & b 
    \end{tabular}
    \caption{Algorithmic a) Energy Consumption Comparison of Individual Algorithms, Pipelines and b) Frames per Second on Heterogenous Platform. CPU(5900x) GPU(A2000) FPGA(Alveo U50).}
    \label{fig:FPSEnergy}
\end{figure}




\section{CONCLUSION}
\label{sec:conclusion}

In conclusion
summarise the main findings of research

discuss the implications of my work and potential future directions

highlight the significance of my results and their impact on the field of heterogeneous vision systems


\section{FUTURE WORK}
\label{sec:future work}

Future research will enhance the efficiency and effectiveness of the scheduling framework for heterogeneous vision systems. We will explore dynamic load balancing and machine learning-based partitioning to optimise task distribution across CPUs, GPUs, and FPGAs by considering real-time performance metrics and resource availability. Machine learning-based partitioning will use historical data and performance models for informed task allocation.

We will investigate the benefits of developing a new C++ Domain-Specific Language (DSL) for optimising computer vision pipelines. Alternatively, we will evaluate the sufficiency of existing Python-embedded libraries like PyTorch with ONNX as the Intermediate Representation (IR), considering ease of use, flexibility, and performance.

We will also focus on heterogeneous dataflow graph optimisations for computer vision algorithms as an extension of TVM. This includes researching subgraph optimisations such as operator fusion, memory layout transformations, and loop tiling to enhance performance and reduce computational overhead.

Efficient data transfer mechanisms will be another focus area. We will develop robust methods to manage dataflow between distinct memory architectures in a heterogeneous system using Direct Memory Access (DMA). This will minimise data transfer latency and maximise throughput, crucial for real-time image processing applications. Our work will include designing custom DMA engines and optimising data transfer protocols.

Additionally, we will develop dynamic resource allocation strategies and optimisation techniques for the deployment framework. These strategies will adapt to varying workloads and resource constraints, ensuring optimal performance and resource utilisation across the heterogeneous system. This involves implementing real-time monitoring and adaptive control mechanisms for dynamic resource allocation.

Practical demonstrations and applications will be crucial. We will showcase the developed heterogeneous system through real-world scenarios, highlighting its energy efficiency and runtime performance. Demonstrations will include tasks such as object detection, image segmentation, and real-time video processing, validating the framework's advantages.

\input{tables/resultsummary}
\input{tables/heterogenousresults}
\input{tables/individualpipelines}

\acknowledgments % equivalent to \section*{ACKNOWLEDGMENTS}       
 
This work was supported in part by the UKRI EPSRC.

% References
\bibliography{report} % bibliography data in report.bib
\bibliographystyle{spiebib} % makes bibtex use spiebib.bst

\end{document} 

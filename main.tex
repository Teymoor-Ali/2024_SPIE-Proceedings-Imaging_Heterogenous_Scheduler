\documentclass[]{spie}  %>>> use for US letter paper
%\documentclass[a4paper]{spie}  %>>> use this instead for A4 paper
%%\documentclass[nocompress]{spie}  %>>> to avoid compression of citations

\renewcommand{\baselinestretch}{1.0} % Change to 1.65 for double spacing

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{comment}
\usepackage{color}
\usepackage{tabularray}
\definecolor{Silver}{rgb}{0.752,0.752,0.752}
\definecolor{Mercury}{rgb}{0.901,0.901,0.901}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{pgfplots}
\usepackage{tikz}
\pgfplotsset{compat=1.17}
\pgfplotsset{compat=1.3}
\usepackage{booktabs}
\usepackage{hhline}
\usepackage{float}
\usepackage[table,xcdraw]{xcolor} % Needed for coloring tables
\usepackage{caption}
\usepackage{tabularray}





\definecolor{Gray1}{gray}{0.9}
\definecolor{Gray2}{gray}{0.8}
\definecolor{ppink}{HTML}{ec81fc}
\definecolor{oorange}{HTML}{eda477}
\definecolor{bbrown}{HTML}{826441}
\definecolor{peridot}{rgb}{0.9, 0.89, 0.0}
\definecolor{richlavender}{rgb}{0.67, 0.38, 0.8}
\definecolor{Silver}{rgb}{0.752,0.752,0.752}

\definecolor{forestgreen}{rgb}{0.0, 0.27, 0.13}
\definecolor{darkcandyapplered}{rgb}{0.64, 0.0, 0.0}
\definecolor{paleplum}{rgb}{0.8, 0.6, 0.8}
\definecolor{pastelorange}{rgb}{1.0, 0.7, 0.28}

\definecolor{yellow-green}{rgb}{0.6, 0.8, 0.2}
\definecolor{wenge}{rgb}{0.39, 0.33, 0.32}
\definecolor{corn}{rgb}{0.98, 0.93, 0.36}
\definecolor{darkpowderblue}{rgb}{0.0, 0.2, 0.6}
\definecolor{dimgray}{rgb}{0.41, 0.41, 0.41}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Scheduling Algorithms on Heterogeneous Architectures for Efficient Vision Systems}

\author[]{Teymoor Ali. }
\author[a,b]{George Paul. }
\author[a]{Deepayan Bhowmik.}
\author[b]{Robert Nicol.}
\affil[a]{Newcastle University, 1 Science Square, Newcastle upon Tyne, U.K}
\affil[b]{STMicroelectronics, 1 Tanfield, Inverleith Row, Edinburgh, U.K}

\authorinfo{Further author information: (Send correspondence to T. Ali)\\T. Ali: E-mail: Teymoor.A@outlook.com, \\  G.P E-mail: George.Paul@newcastle.ac.uk.}

% Option to view page numbers
\pagestyle{empty} % change to \pagestyle{plain} for page numbers   
\setcounter{page}{301} % Set start page numbering at e.g. 301
 
\begin{document} 
\maketitle

\begin{abstract}
Deep Neural Network (DNN) algorithms have become ubiquitous within the vision domain, encompassing various tasks, including object detection, segmentation, and classification. However, executing complex DNN algorithms on real-time vision systems demands better energy efficiency, runtime, and accuracy requirements. The growing demand for image processing algorithms on systems with resource and energy constraints requires architectures that perform tasks efficiently. Traditionally, embedded imaging designs often involve implementing image processing algorithms on homogeneous architectures, which have hardware limitations. This has led to innovative computing architectures, leveraging heterogeneity that combines CPUs, GPUs, FPGAs, and other accelerators into a single processing fabric. This design choice allows applications to utilise the most efficient architecture for a given algorithm.

However, scheduling and partitioning algorithms remain arduous, particularly when distributing operations among accelerators with different computing paradigms. This challenge requires a balance, considering important factors such as computational power, memory bandwidth, and communication overhead. In addition, scheduling tasks involves determining the order in which tasks are executed while considering factors such as task dependencies, resource availability, and synchronisation. Current state-of-the-art deep learning libraries are primarily designed to target single architectures for model deployment and often lack mechanisms to intelligently partition an algorithm's sub-operations across the most suitable processor. This paper presents a scheduler targeting heterogeneous vision systems, carefully fine-grain partitions and maps sub-operations of widely used convolutional neural networks and image processing algorithms. The results show that the heterogeneous platform is $1.11\times$ \& $1.08\times$ more efficient in kernel runtime and energy consumption than their best performing discrete hardware counterparts, GPU and FPGA. The experiments reveal that partitioning algorithms based on their runtime and energy properties on a given accelerator and optimally scheduled perform better in energy and runtime efficiency than their best-performing homogeneous components executing the complete algorithm.
\end{abstract}

% Include a list of keywords after the abstract 
\keywords{Heterogeneous Computing, Computer Vision, FPGA-GPU, Heterogenous Scheduler}


\section{INTRODUCTION}
\label{sec:intro}
Vision systems have increasingly become pervasive in many application domains, particularly those utilising deep learning (DL) algorithms such as Satellites \cite{10.1145/2491956.2462176}, Autonomous Vehicles \cite{kuutti2020survey}, and Remote Sensing \cite{YUAN2020111716}. However, many of these systems face low-SWaP (Size, Weight, and Power) constraints, adding further complexity and limiting available resources. Traditionally, most hardware architectures are composed of homogeneous compute units coupled with fixed memory hierarchies, such as Central Processing Units (CPUs) or Graphics Processing Units (GPUs). While these architectures compute specific algorithms efficiently, in some cases, performance is traded off for flexibility, leading to poorer execution for operations that do not fit the design. This rigidity becomes particularly problematic when deploying deep learning models with diverse computational needs. The diverse operators within deep learning and image processing workloads, ranging from convolutional layers to recurrent networks, often exhibit varying levels of parallelism and memory access patterns.

Heterogeneous architectures have been shown as a potential path forward to keep up with the rising demand for better performance. This hardware paradigm integrates various compute accelerators to form a single compute fabric, sharing resources whilst leveraging their unique processing characteristics. Computer vision algorithms comprise various image processing and deep learning algorithms, forming a pipeline. GPUs, with their massively parallel architecture consisting of thousands of cores, streaming multiprocessors (SMs), and single instruction multiple data (SIMD) units, excel at accelerating pixel-level computations. Their specialised hardware, such as texture units for efficient image filtering and high-bandwidth memory interfaces for fast data access, enables fast execution of pixel processing streams. 

In addition, FPGAs bring their own unique advantages for imaging tasks due to their inherent flexibility and configurable architecture. Unlike fixed-function processors, FPGAs offer the capability to tailor specific image processing algorithms through hardware-level customisation, leveraging programmable logic blocks, custom memory hierarchy, and digital signal processing (DSP) slices. This level of customisation enables the development of highly optimised image processing pipelines finely tuned to specific requirements. Furthermore, FPGAs employ a versatile routing architecture that can be configured to manage dataflow between operations efficiently. This fine-grained control over routing allows data to be directed through the configurable logic fabric with minimal latency. This customisation capability allows for low-latency, real-time processing of image streams. Neural Processing Units often incorporate specialised hardware units, such as systolic arrays or matrix multiplication engines, optimised for the core mathematical operations found in deep neural networks.

Furthermore, as deep neural networks increase in architectural complexity and model size, they demand significant memory resources to store their weights, posing challenges for implementation on resourceâ€”and energy-constrained platforms. Despite these challenges, true heterogeneous computing offers a pathway to developing runtime and power-efficient designs. By exploiting architectures with sufficient resources and processing capabilities, it is possible to achieve high-performance computation while adhering to the constraints of resource-limited environments.

Deep learning frameworks such as TensorFlow\cite{tensorflow2015} and PyTorch\cite{Pytorch} provide high-level APIs and abstractions to simplify model development and deployment. These frameworks utilise graph-based intermediate representations (IR) like XLA or TorchScript to enable optimisations and portability across different hardware platforms. Machine learning compilers like TVM\cite{chen2018} or MLIR\cite{mlir} further optimise these IRs for specific targets, leveraging techniques such as operator fusion, memory layout optimisation, and kernel specialisation. However, the current tools often revolve around a single backend, such as CUDA for GPUs. While this simplifies development for specific hardware, it limits the ability to exploit the heterogeneity. Consequently, limiting further hardware and image domain-specific optimisations\cite{ali2023domain}.


In this paper, we propose a scheduler that partitions computer vision pipelines and their sub-algorithms into suitable accelerators, optimising performance and energy efficiency. Subgraphs are generated from PyTorch for each selected pipeline, allowing distinct segments of the algorithms to be independently executed on various accelerators, thus leveraging the strengths of heterogeneous systems. We evaluate the performance of these partitioned pipelines and compare them to their discrete accelerator counterparts, analysing key metrics such as inference time, energy consumption, and accuracy. Additionally, the scheduler's performance is benchmarked against other ML compilers, such as TVM. The feasibility and benefits of this approach offer key insights into optimal deployment strategies for computer vision pipelines on heterogeneous hardware platforms.

% Convolutional Neural Networks (CNNs)  are prevalent in various domains, including object detection\cite{zhao2012flip}, image classification\cite{rawat2017deep}, and image segmentation\cite{minaee2021image}. These image processing algorithms are generally designed and implemented on GPUs, which feature thousands of compute cores coupled with high-bandwidth memory. This configuration enables efficient execution of Single Instruction, Multiple Data (SIMD) operations, making GPUs ideal for parallel processing of extensive data sets. However, the execution of algorithms on GPUs involves trade-offs in power consumption, physical size, and latency\cite{PouSamSad}.



The main contributions of this paper are as follows:
\begin{itemize}
\item Development of a heterogeneous scheduler for computer vision pipelines, enabling task allocation to a selected compute unit and facilitating direct memory access operations between GPU and FPGA.
\item Optimised deployment of computer vision pipelines on heterogeneous architectures, demonstrating improved throughput and reduced energy consumption compared to homogeneous accelerator solutions.
\item Comprehensive benchmarking and evaluation of various image processing algorithms generated by machine learning frameworks (ONNX, PyTorch) across a heterogeneous system, including CPUs, GPUs, and FPGAs.
\end{itemize}

\newpage


\section{RELATED WORK}
\label{sec:background}



\textbf{Heterogeneous Architectures}. have seen increasing attention towards as an alternative to the limitations of homogeneous silicon chips \cite{RooLav17, KobRyoFuj20, XieLinKai17, ChoLeeLee22, Hosseinabady2019HeterogeneousFE}. Previous studies \cite{QasDenKri29} indicate that not all algorithms are suited to a single type of accelerator. These studies compare the energy efficiency of different architectures for image processing tasks and reveal that GPUs generally consume less energy per frame than CPUs and FPGAs. However, for more complex kernels and complete vision pipelines, FPGAs outperform other accelerators. In addition, CNNs executed on FPGAs are more efficient than those on GPUs for inference tasks, both in terms of energy consumption and, in some cases, processing time, as evidenced by Blott et al. \cite{BloHalLis}.



\noindent\textbf{Partitioning.} algorithms for heterogeneous architectures require careful balancing diverse computing elements and managing resource dependencies\cite{shekhar2015}. One work introduced StreamBlocks, a software-based FPGA compiler that simplifies partitioning tasks across heterogeneous CPU/FPGA platforms but noted the difficulty of achieving an optimal division without extensive profiling and code modifications \cite{emami2022}. Another study addressed the problem of optimising data-parallel applications on heterogeneous HPC platforms by proposing a model-based partitioning algorithm that tackles the intricacies of resource contention and NUMA effects. However, this algorithm requires detailed profiling \cite{khaleghzadeh2018}. Further research emphasised the energy and performance trade-offs in workload partitioning, demonstrating that effective utilisation of heterogeneous processors requires sophisticated balancing strategies \cite{tang2017}. Other research has explored prioritising the energy efficiency of accelerators when partitioning tasks in heterogeneous computing environments. Hamano et al.\cite{Hamano} proposed a power-aware dynamic task allocation strategy for heterogeneous clusters of CPUs and GPUs, aiming to minimise the Energy-Delay Product (EDP). The approach prioritises energy efficiency by initially assigning tasks to the lowest-power resources and then selectively offloading tasks with a high acceleration potential to other resources to reduce processing time. However, this method may lead to excessive energy consumption when minimising processing time, even if the application does not require instant processing. Task partitioning offers a more fine-grained scheduling approach by dividing tasks into sub-tasks and assigning them to suitable resources. Inta et al.\cite{IntBowDav12} presented a static partitioning method for FPGA-GPU hybrid systems. However, such methods can introduce additional delays due to the processing time differences between FPGAs and GPUs, potentially causing the system to miss service deadlines. Oh et al.\cite{OhHanYan18} improved the scheduling method to consider both energy consumption and compliance with the latency limitation of
the application at the same time.

\noindent\textbf{Scheduling.} tasks across heterogeneous systems come with additional communication and memory latency challenges. Scheduling involves efficiently allocating computational tasks across various processing units such as CPUs, GPUs, FPGAs, and other specialised hardware. This scheduling aims to optimise performance, resource utilisation, and energy efficiency by leveraging the distinct strengths of each hardware type. Several works have explored the deployment of algorithms for heterogeneous architectures on edge and HPC domain\cite{KheHardMue24,RatGouHog23}.  Nupur et al. \cite{SumRaw22} presented an optimal partitioning approach to map a CNN model across heterogeneous architectures by leveraging performance measurement benchmarks. The proposed framework, Hetero-vis, decides optimal partitions and mapping of a CNN network across different accelerators to minimise the inference latency. Despite progress, current deep learning libraries often target one backend, lacking mechanisms to partition and distribute tasks across various processors intelligently. This gap requires the development of schedulers that can dynamically allocate resources based on the computational requirements and available hardware\cite{KanLeeKil21, LanBhaSou2016}.

\noindent\textbf{Compiler Frameworks}. such as TVM and PyTorch, employ sophisticated techniques for optimising and transforming deep learning models. TVM utilises graph optimisation, kernel fusion, and hardware-specific code generation to execute efficiently across various hardware platforms \cite{chen2018}. Similarly, PyTorch integrates dynamic computational graphs with JIT compilation and custom kernel optimisation to enhance model performance on diverse architectures \cite{Pytorch}. These compilers aim to bridge the gap between high-level model specification and efficient hardware execution, improving model inference speed and resource utilisation. A framework utilising a polyhedral model was proposed by Baghdadi et al.\cite{BagRiyRay19} to generate high-performance code for many backends. Its design separates computation into four intermediate representations: algorithms, loop transformations, data layouts, and communication. Other approaches employ a two-phase search algorithm, hierarchical sampling, and memorisation of repeated partial schedules to address scalability challenges. These optimisation results have shown compile-time speedups and performance improvement\cite{Halide2018}.


\section{PIPELINE AND SCHEDULER DESIGN}
\label{sec:methodology}

This section details the heterogeneous deployment of computer vision pipelines that combine OpenCV-based image processing with a Convolutional Neural Network (CNN) classifier, utilising PyTorch as the framework for describing the pipeline. 

The computational graph generated from the PyTorch algorithm includes both the image processing and classifier stages. The graph is manually partitioned by the program to allocate tasks across the GPU and FPGA, with both accelerators being utilised for any task based on availability. This approach involves executing image processing algorithms on either the GPU or FPGA and performing inference on whichever accelerator is free, using Direct Memory Access (DMA) to transfer processed image frames between the GPU and FPGA efficiently. The primary objective is to optimise runtime, frames per second (FPS), latency, and energy consumption. This demonstrates that this heterogeneous approach is superior to the traditional homogeneous approach where both image processing and inference are executed entirely on GPU or FPGA.

\subsection{Pipeline Overview}

The computer vision pipeline begins with an image processing module using OpenCV functions, followed by a CNN classifier. Both stages are implemented as a single module in PyTorch. The deployment flow is illustrated in Fig. \ref{fig:deployment_flow}.

The scheduler splits the graph into sub-operations, assigning them to the most suitable hardware units based on their computational characteristics. In addition, the scheduler is integrated into a deployment framework that works with PyTorch, ONNX Runtime, and the vendor compilers for the target hardware including NVCC and Vitis compilers. Furthermore, peer-to-peer direct memory transfer (DMA) is injected into the generated code to transfer image data from FPGA to GPU buffers without involving the host (CPU). The FPGA scheduling flow takes longer when algorithms have to be synthesised in cases where pre-written IP is not used.

% Perhaps table for TVM vs PyTorch to justify our decision
Although TVM 0.6 can process ONNX computation graphs, when tested as part of the scheduler it performed slower in throughput than PyTorch. TVM incurs significant overhead due to its intermediate representation and compilation steps, which may not be optimised for dynamic control flows. Additionally, TVM may struggle with kernel fusion in complex pipelines. Moreover, while TVM generally optimises CNN channels using the GEMM implementation for 2-D convolution, there are frequent instances where it opts for direct convolution, which is typically slower regardless of the target hardware characteristics \cite{9042000}. Furthermore, TVM may not fully support newer GPU architectures, limiting its efficiency and requiring additional tuning to achieve optimal performance.


\begin{figure}[tb]
\centering
\includegraphics[width=\textwidth]{images/SPIE_Deployment_Flow.png} 
\caption{Scheduler Architecture to Partition and Schedule Algorithms onto CPU, GPU, and FPGA.}
\label{fig:deployment_flow}
\end{figure}

\subsection{Pipeline Conversion and Deployment}

Initially, the pipeline is implemented and validated in PyTorch, including image processing with OpenCV functions and a CNN for classification. To optimise execution on CPUs and GPUs, the PyTorch module is converted to TorchScript, which supports various graph-level optimisations while preserving the model structure for efficient execution.

The module is exported to the Open Neural Network Exchange (ONNX) format for deployment on FPGAs. ONNX standardises the model representation, facilitating interoperability across different hardware platforms and leveraging FPGAs' reconfigurable architecture. Additionally, the model undergoes quantisation to reduce precision, optimising power and memory usage, which is particularly beneficial for resource-constrained devices like FPGAs.

\subsection{Heterogeneous Scheduler}
\begin{algorithm}[]
\caption{Heterogeneous Scheduler}
\begin{algorithmic}[1]

\STATE \textbf{function} Scheduler(PythonCode)
\STATE \hspace{1em} Parse PythonCode for pragmas (\texttt{gpu}, \texttt{fpga})
\STATE \hspace{1em} PartitionedCode $\leftarrow$ []

\STATE \hspace{1em} \textbf{for each} function \textbf{in} PythonCode:
\STATE \hspace{2em} \textbf{if} pragma == \texttt{gpu} \textbf{or} pragma == \texttt{fpga}:
\STATE \hspace{3em} \textbf{if} function in PerformanceCost:
\STATE \hspace{4em} \textbf{if} PerformanceCost[function] == \texttt{gpu}:
\STATE \hspace{5em} GeneratedCode $\leftarrow$ GenerateGPUCode(function)
\STATE \hspace{4em} \textbf{else if} PerformanceCost[function] == \texttt{fpga}:
\STATE \hspace{5em} \textbf{if} PreOptimizedLibraryExists(function):
\STATE \hspace{6em} GeneratedCode $\leftarrow$ UsePreOptimizedLibrary(function)
\STATE \hspace{5em} \textbf{else}:
\STATE \hspace{6em} ONNXCode $\leftarrow$ ConvertToONNX(function)
\STATE \hspace{6em} GeneratedCode $\leftarrow$ CompileWithVitis(ONNXCode)
\STATE \hspace{5em} DMAConfig $\leftarrow$ AddDMAToFPGA(GeneratedCode)
\STATE \hspace{4em} Append PartitionedCode with GeneratedCode and DMAConfig
\STATE \hspace{1em} \textbf{return} PartitionedCode

\end{algorithmic}
\end{algorithm}


The scheduler is designed to partition Python code and execute operations on GPU and FPGA. To reduce the host communication overhead, the algorithm incorporates remote direct memory access\cite{xilinx_2019, nvidia_2024}. It starts by parsing the input Python code to detect pragmas, explicitly looking for \texttt{\#GPU} or \texttt{\#FPGA} directives that indicate the preferred execution platform for each function. If no directives are present, the algorithm defaults to using an internal cost model file, which includes a list of OpenCV functions and specifies the most suitable accelerator for each function. The scheduler employs a FIFO (First In, First Out) approach to ensure that the algorithms are executed in the order they appear in the code. As the scheduler parses the input code, it sequentially processes each function according to its pragma directives or performance cost evaluations. The first algorithm encountered is executed first, and the resulting data is then passed on to the following algorithm in line. This method ensures a streamlined and orderly execution flow, where the output from one function serves as the input for the subsequent function, maintaining data continuity and reducing complex data dependency.

For functions marked with a \texttt{\#GPU} pragma, or according to the performance cost evaluation, the scheduler invokes Pytorch to process the computational graph to generate GPU executable code.
On the other hand, for functions marked with an \texttt{\#FPGA} pragma, the algorithm checks for the availability of pre-optimised libraries (e.g. Vitis Vision Library). If a pre-optimised library is found, the \texttt{UsePreOptimizedLibrary} function is called to retrieve and use the library code, thereby reducing compilation and synthesis time. The code is converted to ONNX computational graph format for TVM flow if no such function exists in the library. The computational graph generated from TVM is synthesised using the Vitis compiler to produce register-transfer-level logic.
DMA logic and configurations are also manually added to both FPGA and GPU. This step ensures high-speed data transfer with minimal CPU involvement, leveraging DMA's ability to move data directly between memory and peripherals. 

\subsection{Algorithm Profiling \& Partitioning Strategy}
To find the most optimal accelerator, each sub-algorithm within a computer vision is profiled manually to develop a primitive cost model. Each algorithm is executed on a CPU, GPU and FPGA and their profiling runtime, FPS and energy consumption for each hardware accelerator are recorded in Fig. \ref{fig:IndividualRuntime}, Fig. \ref{fig:FPSEnergy} and Table \ref{tab:Individualsummary}. 

\begin{figure}
    \centering
\resizebox{0.7\linewidth}{!}{\input{graphs/PartitionStrat}}    %\caption{Execution times for individual and combined algorithms.}
    \caption[SIFT Power Consumption]{Kernel (Lighter Colour) and Total Runtime (Solid Colour) Comparison of Individual Algorithms and Pipelines on each Hardware Platform. CPU(5900X) GPU(A2000) FPGA(Alveo U50).}
    \label{fig:IndividualRuntime}
\end{figure}

Regarding individual algorithm kernel runtimes, the results reveal that CPUs suffer poorly on high-complexity algorithms requiring more parallelisation. However, algorithms such as \texttt{RGB2Gray} and \texttt{Resize} exhibit better performance on the CPU than the GPU. The lower GPU runtime can be attributed to communication latency when allocating cores takes longer than the processing. The FPGA is shown to perform $1.81\times$ faster than the GPU and $1.79\times$ faster than the CPU in terms of kernel time for \texttt{RGB2GRAY}, \texttt{ Histogram Equalisation} and \texttt{Resize}) algorithms. However, the FPGA is less efficient for the \texttt{Gaussian} Filter algorithm, performing $1.2\times$ slower than the GPU. This inefficiency arises due to the limited number of logic blocks available on the FPGA, which are insufficient to fully parallelise the processing of all pixels. As a result, the FPGA cannot leverage its parallel processing capabilities to the same extent as the GPU, leading to longer kernel runtimes. This highlights a critical trade-off point where the FPGA's hardware constraints limit its performance on more demanding algorithms. In the case of deep learning algorithms, the GPU is $4.21\times$ \& $1.41\times$ faster than the CPU and FPGA, respectively. The convolutional layer, which contains the highest intensity of the operations in a CNN, benefits from the many-core GPU architecture. 

Slow GPU performance is observed for low-complexity algorithms where parallelisation cannot be effectively exploited. On the other hand, the bitstream pre-initialised U50 FPGA waits for incoming data, resulting in minimal latency, which can benefit image sensors streaming pixel data. In addition, the model initialisation takes, on average, $150ms$ on both CPU and GPU. Moreover, the U50 employs high-bandwidth memory, which can further improve memory-bound algorithms instead of the GDDR6 of the A2000 GPU. The runtime disparity between the \texttt{Total} and \texttt{Kernel} runtimes is due to the setup and initialisation times, including model/weight loading, computation graphs generation, memory allocation and data preprocessing. Therefore, GPU performs poorly, taking device and model initialisation into account, but this will only occur once.



According to Fig. \ref{fig:FPSEnergy}, the Frames Per Second (FPS) results show the impact of CNN algorithms. Large CNN architectures (e.g., Resnet-50 and InceptionV3) with deeper layers will involve many more operations, and additional processing modules and residual connections will also affect overall performance. In relation to energy consumption, the FPGA consumed the least amount of energy per operation for all algorithms. On average, the FPGA is $7.63\times$ and $2.60\times$ more energy efficient than the CPU and GPU. Although the GPU has a higher thermal design power (TDP) than the CPU, the time it takes to execute the algorithm is much lower, resulting in lower overall energy use despite the higher power draw. Algorithms that contain many operations, such as ML and Gaussian, show that the GPU consumes less energy than the CPU.

Referring to the profiling results, the partitioning approach will place the pre-processing algorithms on the FPGA to maximise energy efficiency. In contrast, the deep learning algorithm will be deployed on the GPU for runtime. This strategy takes advantage of the FPGA's low initialisation latency and memory coupled with the GPUs parallel processing capability.




\begin{figure}[tb]
    \centering
    \begin{tabular}{cc}
    \resizebox{0.5\columnwidth}{!}{\input{graphs/IndividualEnergy}} &
    \resizebox{0.5\columnwidth}{!}{\input{graphs/IndividualFPS}} \\    a & b 
    \end{tabular}
    \caption{Algorithmic a) Energy Consumption Comparison of Individual Algorithms, Pipelines and b) Frames per Second on each Hardware Platform. CPU(5900x) GPU(A2000) FPGA(Alveo U50).}
    \label{fig:FPSEnergy}
\end{figure}



% \begin{figure}
%     \centering
% \resizebox{0.7\linewidth}{!}{\input{graphs/IndividualEnergy}}    %\caption{Execution times for individual and combined algorithms.}
%     \caption[SIFT Power Consumption]{Energy Consumption Comparison of Individual Algorithms and Pipelines on each Hardware Platform.}
%     \label{fig:IndividualPower}
% \end{figure}




\section{Experimental Setup}
\input{tables/PlatformSummary} 
The proposed partitioning is evaluated using a heterogeneous platform containing various hardware components and their software, as shown in Table \ref{tab:HWEnvironment} and Fig. \ref{fig:HeterogenousHardware}. The heterogeneous platform, integrates a CPU, GPU and FPGA that are connected with a PCIe Gen3 interface. The operating system used for the experiment is Linux Ubuntu with loaded DMA drivers provided by the manufacturer.

\textbf{Dataset.} The test images used in the experiments are from LIU4K-v2 dataset \cite{LiuliuYan19}, which is a high resolution data-set that includes 2000 $3840\times2160$ images. The images contain a variety of backgrounds and objects. 


\subsection{Measurement Metrics}
\subsubsection{Execution time}
The evaluation of overall system performance considers both latency and compute factors, reporting performance metrics for total time, inference, and other significant layers while using floating-point 16-bit precision. The runtime is measured using the host platform's built-in time libraries. The algorithm runtime is collected from executing $1000\times$ and averaged.  

The U50 FPGA, time measurements are captured using the Xilinx Vitis analyser built-in performance counters. On the GPU, CUDA events are utilised to measure time. CUDA events work by recording timestamps at specific points in the code. These timestamps are captured by calling \verb|cudaEventRecord()| at the code's desired start and stop points. The \verb|cudaEventElapsedTime()| function is then used to calculate the elapsed time between these two events. CUDA events provide high-resolution timing by using the GPU's internal clock, allowing for precise measurement of kernel execution times and other GPU operations with minimal overhead. For the CPU, a C++ library is employed to measure execution time. This library offers high-resolution timing functions such as \verb|std::chrono::high_resolution_clock| to capture start and end times, providing precise timing information for the CPU operations. The frame per second (FPS) metric is computed using Eq. \ref{eq:FPS}: 

\begin{equation}\label{eq:FPS}
\text{FPS}= 1/\text{Execution Time}.
\end{equation}

\subsubsection{Power Consumption}

Taking a device's instantaneous power or TDP is inaccurate since power consumption varies depending on the workload. Therefore, measuring power over the time it takes for the algorithm to execute improves accuracy as opposed to using just fixed Wattage. A script is developed to start and stop the measurements during the algorithm's execution. The mean wattage is averaged and multiplied by the time to determine the energy consumed in Joules (J). The energy consumption is obtained using Eq. (\ref{eq:Energy}), where \textit{E} is energy, \textit{P} represents power, and \textit{t} time.

\begin{equation}\label{eq:Energy}
E = P \times t
\end{equation}

Power measurements are taken using software-based platform-specific tools. For the CPU, \texttt{RAPL} collects energy consumption using a script that reads the initial and final energy values. For the GPU, \texttt{nvidia-smi} obtains power readings at the same interval. For the U50 FPGA, \texttt{xbutil} monitors power consumption. The power readings are then averaged to provide an accurate measure of energy consumption during the algorithm's execution.

\subsubsection{Accuracy}
The accuracy of the proposed system for CNN classification is evaluated using a straightforward metric: the ratio of correctly classified images to the total number of images in the test dataset. This metric provides a clear measure of how well the CNN model performs in terms of correctly identifying the class labels of the input images. The accuracy is calculated using Eq. \ref{eq
}:

\begin{equation}\label{eq
}
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
\end{equation}

% RMSE is used to compare the accuracy of the output images from each algorithm, across hardware platforms. RMSE quantifies the differences between predicted and actual pixel values in image processing tasks. It is calculated using Eq. \ref{RSME}:

% \begin{equation}\label{RSME}
% \text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (I_{pred}(i) - I_{actual}(i))^2}
% \end{equation}

% where \( N \) is the total number of pixels, \( I_{\text{pred}}(i) \) is the predicted pixel value, and \( I_{\text{actual}}(i) \) is the actual pixel value.



\section{Results}
\begin{figure}[tb]
    \centering
    \begin{tabular}{cc}
    \resizebox{0.48\columnwidth}{!}{\input{graphs/HeterogenousResults}} &
    \resizebox{0.49\columnwidth}{!}{\input{graphs/HeterogenousEnergy}} \\    a & b 
    \end{tabular}
    \caption{a) Runtime Comparison of Pipeline Algorithms and b) Energy Consumption on a Heterogeneous Platform. CPU(5900x) GPU(A2000) FPGA(Alveo U50).}
    \label{fig:HeteroResult}
\end{figure}



The results in Fig. \ref{fig:HeteroResult} show the selected vision pipelines total and kernel run times and the energy consumption of these algorithms on a heterogeneous platform.

In kernel runtimes, it can be observed that \textit{InceptionV3} takes the longest to compute ($8.44ms$) due to its inception module architecture, which contains multiple parallel convolutional layers with different filter sizes. In contrast, \textit{MobilnetV2} executes in the least amount of time ($5.22ms$), which can be attributed to the efficient depthwise separable convolutions. The kernel runtime results indicate that the heterogeneous platform outperforms the other discrete accelerators (CPU,GPU,FPGA) by $4.20\times$, $1.12\times$, and $1.43\times$, correspondingly.  As for the speedup, heterogeneous architecture achieves a speedup of \(4.19\times\) over the CPU, \(1.12\times\) over the GPU, and \(1.42\times\) over the FPGA. In respect to kernel energy, the heterogeneous platform demonstrates notable efficiency, averaging \(10.11\times\) more efficient than the CPU and \(1.27\times\), \(1.34\times\) for FPGA and GPU.

In respect of total runtime, the \texttt{Hist.+ResNet-50} and \texttt{Gauss.+MobilnetV2} pipeline shows the longest runtime on the discrete GPU, whilst the FPGA achieves the shortest time for all pipelines due to its immediate operation upon bitstream configuration. In the \texttt{R2G+InceptionV3}, the CPU starts to lag behind the GPU with a marginally longer device and model initialisation period. 

In context to total energy, heterogeneous platform is \(1.15\times\) more energy-efficient than the CPU, \(1.09\times\) more efficient than the GPU, but slightly less efficient than the FPGA for the \texttt{Hist.+ResNet-50} pipeline. For the \texttt{Gauss.+MobilnetV2} pipeline, the heterogeneous platform is \(1.56\times\) more energy-efficient than the CPU but slightly less efficient than the GPU and FPGA. The \texttt{R2G+InceptionV3} pipeline shows the heterogeneous platform being \(1.53\times\) more energy-efficient than the CPU, though slightly less efficient than the GPU and FPGA. These results highlight the heterogeneous platform's significant energy efficiency benefits over individual accelerators.


It is important to consider the full data/memory transfer latency for total execution time, as both heterogeneous architecture implementations are impacted by interconnect (PCIe) and distance bottlenecks. The accumulated latency from memory transfers between accelerators in convolutional neural networks underscores the need for optimised data transfer and task partitioning strategies to sustain performance. Future hardware advancements incorporating on-chip accelerator integration could diminish latency due to physical proximity and streamline data transfer protocols to enhance processing efficiency. While GPUs, being homogeneous systems, often experience higher memory transfer and initialisation delays, heterogeneous systems can effectively conceal these delays. 

\label{sec:results}

% \begin{figure}
%     \centering
% \resizebox{0.7\linewidth}{!}{\input{graphs/HeterogenousResults}}  
%     \caption{Kernel and Total Runtime  Comparison of Individual Algorithms and Pipelines on each Hardware Platform. CPU(5900X) GPU(A2000) FPGA(Alveo U50).}
%     \label{fig:HeterogeneousRuntime}
% \end{figure}



\section{CONCLUSION}
\label{sec:conclusion}

In this paper, a scheduler is introduced to partition complex computer vision pipelines, which consist of pre-processing algorithms and convolutional neural networks, namely, \textit{Resnet50}, \textit{Mobilnetv2}, and \textit{InceptionV3}, onto a heterogeneous platform. The scheduler parses image processing code and selects the most suitable accelerator for execution based on a performance cost model or accelerator directives. This model is developed by profiling various imaging algorithms on various accelerators to determine their runtime and energy characteristics. The scheduler leverages PyTorch, ONNX, and TVM to generate computation graphs and produce code targeting GPU and FPGA accelerators. The partitioning and scheduling strategy considers each type of accelerator's unique strengths and weaknesses, optimising for both speed and energy efficiency. 

Experimental results demonstrate the schedulers effectiveness, with the heterogeneous platform consistently outperforming individual accelerators in energy efficiency and runtime across various vision pipelines. On average, the heterogeneous platform achieves significant energy savings, being approximately \(10.11\times\) more efficient than the CPU, \(1.27\times\) more efficient than the FPGA, and \(1.34\times\) more efficient than the GPU. The findings suggest that partitioning and scheduling vision pipelines based on runtime and energy profiles holds the potential for efficient deployment on heterogeneous architectures, offering a viable alternative to GPU/FPGA-only applications. This approach maximises computational resources and significantly reduces energy consumption, making it highly suitable for real-time image processing tasks in resource-constrained environments.


\section{FUTURE WORK}
\label{sec:future work}

Future research will focus on enhancing the scheduling framework for heterogeneous vision systems and optimising task distribution across CPUs, GPUs, and FPGAs using dynamic load balancing and machine learning-based partitioning. This will involve real-time performance metrics and resource availability, leveraging historical data and performance models for informed task allocation. In the scope of convolutional neural network algorithm, layer by layer partitioning on the most optimised processes can be applied to further optimise overall performance of CNN algorithms. To further enhance the efficiency, research will explore integrating emerging technologies such as neural processing units and neuromorphic processors. 

Furthermore, developing a heterogeneity inspired Domain-Specific Language (DSL) for optimising computer vision pipelines will be explored, aiming to provide a higher abstraction and efficient approach to designing and implementing vision algorithms. Domain-specific compilers would be extended to target heterogeneous back-ends with data transfers between accelerators.The scheduler is limited by the manual implementation of direct memory access for each generated code, therefore improvements will involve automatically integrating DMA code, eliminating the bottleneck during the compilation/synthesis process.





\input{tables/resultsummary}
\input{tables/heterogenousresults}
\input{tables/individualpipelines}

\acknowledgments % equivalent to \section*{ACKNOWLEDGMENTS}       
 
This work was partly supported by Newcastle University \& STMicroelectronics Imaging Division.

% References
\bibliography{report} % bibliography data in report.bib
\bibliographystyle{spiebib} % makes bibtex use spiebib.bst

\end{document} 
